{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehuseynov/ITU-BLG454-HW1/blob/main/454HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name Surname: Emil Huseynov\n",
        "\n",
        "Student No: 150210906\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "dNlZd5pyw-OR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries to be used\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "W0aHEvh1b6fj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H6-FpGne92Tr"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, f1_score, root_mean_squared_error\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build your own code-base (30 points)\n",
        "\n",
        "Implement the methods provided and compare your implementation with Sklearn library\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ijQ4Fvfvcmez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Nearest Neighbour (5 points)"
      ],
      "metadata": {
        "id": "LguwidLjdgya"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "F33-UFA-bper"
      },
      "outputs": [],
      "source": [
        "class KNN:\n",
        "    def __init__(self, train_data, train_label, k=3):\n",
        "        self.k = k\n",
        "        self.train_data = train_data\n",
        "        self.train_label = train_label\n",
        "\n",
        "    def predict(self, test_data):\n",
        "        predictions = []\n",
        "\n",
        "        for test_point in test_data:\n",
        "            # Calculate the Euclidean distance between the test point and each training data point\n",
        "            distances = []\n",
        "            for i in range(len(self.train_data)):\n",
        "                distance = np.sqrt(np.sum((self.train_data[i] - test_point) ** 2))\n",
        "                distances.append((distance, self.train_label[i]))\n",
        "\n",
        "            # Sort distances and get the k nearest labels\n",
        "            distances.sort(key=lambda x: x[0])\n",
        "            k_nearest_labels = [label for _, label in distances[:self.k]]\n",
        "\n",
        "            # Manually count occurrences to find the most common label\n",
        "            label_counts = {}\n",
        "            for label in k_nearest_labels:\n",
        "                if label in label_counts:\n",
        "                    label_counts[label] += 1\n",
        "                else:\n",
        "                    label_counts[label] = 1\n",
        "\n",
        "            # Determine the label with the highest count\n",
        "            most_common_label = max(label_counts, key=label_counts.get)\n",
        "            predictions.append(most_common_label)\n",
        "\n",
        "        return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "| Feature               | My KNN Implementation            | Sklearn KNeighborsClassifier           |\n",
        "|-----------------------|--------------------------------------|----------------------------------------|\n",
        "| **Distance Metric**   | Only Euclidean                      | Multiple (Euclidean, Manhattan, etc.)  |\n",
        "| **Voting**            | Majority voting only                | Supports weighted voting               |\n",
        "| **Ease of Use**       | Manual setup, basic Python          | Easy-to-use API with `.fit()` & `.predict()` |\n",
        "| **Flexibility**       | Limited options, no customization   | Customizable (metrics, weights)        |\n",
        "| **Performance**       | Slower, unoptimized                 | Fast, optimized with data structures like KDTree |\n",
        "| **Error Handling**    | Minimal error checks                | Robust input validation                |"
      ],
      "metadata": {
        "id": "PfTTdcO9QK-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gaussian** Naive Bayes (5 points)"
      ],
      "metadata": {
        "id": "2EW4yvZlmOPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNB:\n",
        "    def __init__(self, train_data, train_label):\n",
        "        self.train_data = np.array(train_data)\n",
        "        self.train_label = np.array(train_label)\n",
        "        self.classes = np.unique(self.train_label)\n",
        "        self.means = {}\n",
        "        self.variances = {}\n",
        "        self.priors = {}\n",
        "\n",
        "    def gaussian_probability(self, x, mean, variance):\n",
        "        # Calculate the Gaussian probability density function\n",
        "        exponent = np.exp(-((x - mean) ** 2) / (2 * variance))\n",
        "        return (1 / np.sqrt(2 * np.pi * variance)) * exponent\n",
        "\n",
        "    def fit(self):\n",
        "        # Calculate mean, variance, and prior for each class\n",
        "        for c in self.classes:\n",
        "            # Filter data by class\n",
        "            class_data = self.train_data[self.train_label == c]\n",
        "            # Calculate mean and variance for each feature in the class\n",
        "            self.means[c] = np.mean(class_data, axis=0)\n",
        "            self.variances[c] = np.var(class_data, axis=0)\n",
        "            # Calculate prior probability for the class\n",
        "            self.priors[c] = class_data.shape[0] / self.train_data.shape[0]\n",
        "\n",
        "    def predict(self, test_data):\n",
        "        test_data = np.array(test_data)\n",
        "        predictions = []\n",
        "\n",
        "        for x in test_data:\n",
        "            class_probabilities = {}\n",
        "            for c in self.classes:\n",
        "                # Start with the prior probability for the class\n",
        "                class_prob = np.log(self.priors[c])\n",
        "\n",
        "                # Multiply by the Gaussian probability of each feature\n",
        "                for i in range(len(x)):\n",
        "                    mean = self.means[c][i]\n",
        "                    variance = self.variances[c][i]\n",
        "                    class_prob += np.log(self.gaussian_probability(x[i], mean, variance))\n",
        "\n",
        "                class_probabilities[c] = class_prob\n",
        "\n",
        "            # Choose the class with the highest probability\n",
        "            predictions.append(max(class_probabilities, key=class_probabilities.get))\n",
        "\n",
        "        return predictions\n"
      ],
      "metadata": {
        "id": "wXCwTb64dffx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "| Feature               | My GNB Implementation           | Sklearn GaussianNB                     |\n",
        "|-----------------------|-------------------------------------|----------------------------------------|\n",
        "| **Distribution**      | Gaussian (Normal) only             | Gaussian (Normal) only                 |\n",
        "| **Parameter Calculation** | Manual mean, variance, and prior probability calculation | Automatic mean, variance, and prior calculation |\n",
        "| **Ease of Use**       | Requires manual setup              | Simple API with `.fit()` & `.predict()` |\n",
        "| **Flexibility**       | Limited, supports only Gaussian distribution | Can integrate with Sklearn pipeline and additional options |\n",
        "| **Performance**       | Slower on larger datasets, unoptimized | Optimized with C libraries for faster processing |\n",
        "| **Error Handling**    | Basic, minimal error handling      | Robust input validation and error handling |"
      ],
      "metadata": {
        "id": "02vlcFpKQ5ai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (5 points)"
      ],
      "metadata": {
        "id": "8DtdaqINoWd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PCA:\n",
        "  def __init__(self, data, n_components):\n",
        "    self.data = data\n",
        "    self.n_components = n_components\n",
        "\n",
        "  def fit(self):\n",
        "    # implement here\n",
        "    pass\n",
        "\n",
        "  def transform(self,x):\n",
        "    # implement here\n",
        "    pass"
      ],
      "metadata": {
        "id": "0fAfE2jboV_v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metrics (2+3 points)\n",
        "\n"
      ],
      "metadata": {
        "id": "Wp95iB6qffg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse(y_true, y_pred):\n",
        "  # implement here\n",
        "  pass\n",
        "\n",
        "def accuracyNf1_score(y_true, y_pred):\n",
        "  # implement here\n",
        "  pass\n"
      ],
      "metadata": {
        "id": "8L4zDi-lffK2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization Tools (5 points)"
      ],
      "metadata": {
        "id": "kV0sVRDS8j-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### fill here ###"
      ],
      "metadata": {
        "id": "_Kf9PZFy8cYP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validate your implementation using libraries (5 points)\n",
        "\n",
        "(you can generate synthetic data using numpy of import another toy dataset from sklearn)\n"
      ],
      "metadata": {
        "id": "wEdKWex2pcwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### fill here ###"
      ],
      "metadata": {
        "id": "xAqgVs5dp9pz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments (45 points)\n",
        "\n",
        "Use Sklearn classes\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "gvTTC7_BqMmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset preparation"
      ],
      "metadata": {
        "id": "1U41fPKBr1Xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset and split to train and test set\n",
        "digits = load_digits()\n",
        "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=454)\n",
        "\n",
        "# Calculate the frequency of each class in the training set\n",
        "unique_classes_test, class_counts_test = np.unique(y_test, return_counts=True)\n",
        "unique_classes_train, class_counts_train = np.unique(y_train, return_counts=True)\n",
        "\n",
        "for cls_train, count_train, cls_test, count_test in zip(unique_classes_train, class_counts_train, unique_classes_test, class_counts_test):\n",
        "  print(f\"Class {cls_train}: {count_train} train  {count_test} test {count_train/count_test} ratio\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLziE30kX8Nr",
        "outputId": "d1fd9e07-ff14-4f93-b8e0-8a3a6c17dbc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0: 144 train  34 test 4.235294117647059 ratio\n",
            "Class 1: 137 train  45 test 3.0444444444444443 ratio\n",
            "Class 2: 150 train  27 test 5.555555555555555 ratio\n",
            "Class 3: 145 train  38 test 3.8157894736842106 ratio\n",
            "Class 4: 145 train  36 test 4.027777777777778 ratio\n",
            "Class 5: 144 train  38 test 3.789473684210526 ratio\n",
            "Class 6: 146 train  35 test 4.171428571428572 ratio\n",
            "Class 7: 142 train  37 test 3.8378378378378377 ratio\n",
            "Class 8: 143 train  31 test 4.612903225806452 ratio\n",
            "Class 9: 141 train  39 test 3.6153846153846154 ratio\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply classification methods using the dataset directly (10 points)\n",
        "(paramtre denemeleri tarzı şeyler)"
      ],
      "metadata": {
        "id": "W1E9IPBAr9d6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### fill here ###"
      ],
      "metadata": {
        "id": "FYSdPF-mqJhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply PCA and find optimal #components with the best recontruction (RMSE) as the objective (10 points)"
      ],
      "metadata": {
        "id": "IbBlC1WmsXDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### fill here ###"
      ],
      "metadata": {
        "id": "yyOsGnlksrcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply PCA class-wise and merge the transformed features (10 points)"
      ],
      "metadata": {
        "id": "h1CCYucCstUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### fill here ###"
      ],
      "metadata": {
        "id": "ewfFBqaesskI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply classification methods on the transformed features (PCA outputs) both nomral and class-wise (15 points)\n",
        "(normal PCA dimension 30, class-wise PCA dimension 3x10)"
      ],
      "metadata": {
        "id": "hIVgxZwFtJfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### fill here ###"
      ],
      "metadata": {
        "id": "bbx-HOP6tI93"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}